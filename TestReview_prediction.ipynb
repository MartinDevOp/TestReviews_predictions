{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestReview_prediction.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1UkftUvq83KB-Ui2nsiYhEvDwymkR3E0c","authorship_tag":"ABX9TyO8DNUkzoQkW7k/VogSBzvb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BWEyZFv7-2bTn7efIgs877zDn2OY5Xee"},"id":"BudJqId3U0-6","executionInfo":{"status":"ok","timestamp":1660525592743,"user_tz":300,"elapsed":822018,"user":{"displayName":"Martin Ugbala","userId":"13274003947054467308"}},"outputId":"03d8a820-bc25-44fd-fb1d-cda23fc593ab"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","import re, nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('wordnet') \n","import numpy as np\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import pickle\n","import umap\n","import plotly.graph_objs as go\n","from sklearn.svm import LinearSVC\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","import joblib\n","\n","# Reading dataset as dataframe\n","df = pd.read_csv(\"/content/drive/MyDrive/SpotifyReviews.csv\", encoding = \"ISO-8859-1\") # You can also use \"utf-8\"\n","pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n","pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n","# Cleaning Tweets\n","def cleaner(review):\n","    soup = BeautifulSoup(review, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n","    souped = soup.get_text()\n","    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n","    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n","\n","    \"\"\"\n","    For more info on regular expressions visit -\n","    https://docs.python.org/3/howto/regex.html\n","    \"\"\"\n","\n","    tokens = nltk.word_tokenize(re2)\n","    lower_case = [t.lower() for t in tokens]\n","\n","    stop_words = set(stopwords.words('english'))\n","    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n","\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n","    return lemmas\n","\n","df['cleaned_review'] = df.Review.apply(cleaner)\n","df = df[df['cleaned_review'].map(len) > 0] # removing rows with cleaned reviews of length 0\n","print(\"Printing top 5 rows of dataframe showing original and cleaned reviews....\")\n","print(df[['Review','cleaned_review']].head())\n","df.drop(['Review'], axis=1, inplace=True)\n","# Saving cleaned tweets to csv\n","df.to_csv('cleaned_data.csv', index=False)\n","df['cleaned_review'] = [\" \".join(row) for row in df['cleaned_review'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n","data = df['cleaned_review']\n","Y= df['Recommend'].map({'Yes':1,'No':0})\n","#Y = df.Recommend # target column\n","print('Out of 100%, nearly {}% belongs to positive class'.format(round(sum(Y/len(Y)*100))))\n","\n","tfidf = TfidfVectorizer(min_df=.00084998, ngram_range=(1,3)) # min_df=.00084998 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering\n","tfidf.fit(data) # learn vocabulary of entire data\n","data_tfidf = tfidf.transform(data) # creating tfidf values\n","pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)\n","print(\"Shape of tfidf matrix: \", data_tfidf.shape)\n","\n","\n","# Implementing Support Vector Classifier\n","svc_clf = LinearSVC() # kernel = 'linear' and C = 1\n","\n","# Running cross-validation\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n","scores=[]\n","iteration = 0\n","for train_index, test_index in kf.split(data_tfidf, Y):\n","    iteration += 1\n","    print(\"Iteration \", iteration)\n","    X_train, Y_train = data_tfidf[train_index], Y.iloc[train_index]\n","    X_test, Y_test = data_tfidf[test_index], Y.iloc[test_index]\n","\n","    svc_clf.fit(X_train, Y_train) # Fitting SVC\n","    Y_pred = svc_clf.predict(X_test)\n","    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n","    print(\"Cross-validation accuracy: \", score)\n","    scores.append(score) # appending cross-validation accuracy for each iteration\n","svc_mean_accuracy = np.mean(scores)\n","print(\"Mean cross-validation accuracy: \", svc_mean_accuracy, \" \\n\")\n","\n","# Implementing Naive Bayes Classifier\n","nbc_clf = MultinomialNB()\n","\n","# Running cross-validation\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n","scores=[]\n","iteration = 0\n","for train_index, test_index in kf.split(data_tfidf, Y):\n","    iteration += 1\n","    print(\"Iteration \", iteration)\n","    X_train, Y_train = data_tfidf[train_index], Y.iloc[train_index]\n","    X_test, Y_test = data_tfidf[test_index], Y.iloc[test_index]\n","    nbc_clf.fit(X_train, Y_train) # Fitting NBC\n","    Y_pred = nbc_clf.predict(X_test)\n","    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n","    print(\"Cross-validation accuracy: \", score)\n","    scores.append(score) # appending cross-validation accuracy for each iteration\n","nbc_mean_accuracy = np.mean(scores)\n","print(\"Mean cross-validation accuracy: \", nbc_mean_accuracy)\n","\n","if svc_mean_accuracy > nbc_mean_accuracy:\n","  clf = LinearSVC().fit(data_tfidf, Y)\n","  joblib.dump(clf, 'svc.sav')\n","else:\n","  clf = MultinomialNB().fit(data_tfidf, Y)\n","  joblib.dump(clf, 'nbc.sav')\n","\n","####################################################\n","## Retraining the selected model for production\n","X = data_tfidf\n","svc = LinearSVC() \n","svc.fit(X, Y)\n","### Saving and loading the model\n","with open('model.pkl', 'wb') as f:\n","    pickle.dump((svc, tfidf), f)\n","with open('model.pkl', 'rb') as f:\n","    model, vect = pickle.load(f)\n","### Testing the model\n","test_data = pd.read_csv(\"/content/drive/MyDrive/TestReviews.csv\")\n","print(test_data)\n","test_data['cleaned_review'] = test_data['Review'].apply(cleaner)\n","test_data= test_data[test_data['cleaned_review'].map(len) > 0] # removing rows with cleaned reviews of length 0\n","print(\"Printing top 5 rows of dataframe showing original and cleaned reviews....\")\n","print(test_data[['Review','cleaned_review']].head())\n","test_data.drop(['Review'], axis=1, inplace=True)\n","# Saving cleaned tweets to csv\n","test_data.to_csv('cleaned_data2.csv', index=False)\n","test_data['cleaned_review'] = [\" \".join(row) for row in test_data['cleaned_review'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n","x = vect.transform(test_data['cleaned_review'])\n","pred = model.predict(x)\n","test_data['predictions'] = pred\n","test_data.head()\n","test_data.to_csv(\"/content/drive/MyDrive/TestReviews.csv\", index = False)\n","\n","## n_neighbor = 150 min_dis = 0.4\n","u = umap.UMAP(n_components=2, n_neighbors=150, min_dist=0.4)\n","x_umap = u.fit_transform(X)\n","data_ = [go.Scatter(x=x_umap[:,0], y=x_umap[:,1], mode='markers',\n","                    marker = dict(color=test_data['predictions'], colorscale='Rainbow', opacity=0.5), #df['Recommend']\n","                                text=[f'cleaned_review: {a}<br>Class: {b}' for a,b in list(zip(test_data['cleaned_review'],test_data['predictions']))],hoverinfo='text')\n","        ]\n","\n","layout = go.Layout(title = 'UMAP Dimensionality Reduction', width = 1000, height = 1000,\n","                    xaxis = dict(title='First Dimension'),\n","                    yaxis = dict(title='Second Dimension'))\n","fig = go.Figure(data=data_, layout=layout)\n","fig.show()\n","\n","\n","## n_neighbor = 200 min_dis = 0.6\n","u = umap.UMAP(n_components=2, n_neighbors=200, min_dist=0.6)\n","x_umap = u.fit_transform(X)\n","data_ = [go.Scatter(x=x_umap[:,0], y=x_umap[:,1], mode='markers',\n","                    marker = dict(color=test_data['predictions'], colorscale='Rainbow', opacity=0.5),\n","                                text=[f'cleaned_review: {a}<br>Class: {b}' for a,b in list(zip(test_data['cleaned_review'],test_data['predictions']))],hoverinfo='text')\n","        ]\n","\n","layout = go.Layout(title = 'UMAP Dimensionality Reduction', width = 1000, height = 1000,\n","                    xaxis = dict(title='First Dimension'),\n","                    yaxis = dict(title='Second Dimension'))\n","fig = go.Figure(data=data_, layout=layout)\n","fig.show()\n","\n","\n","\n"]}]}